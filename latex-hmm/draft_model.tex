%! TEX root = main-hmm.tex

\section{Hidden Markove Model}

%-------------------------------------------------------------------------------
% Defining the hidden Markov model.
%-------------------------------------------------------------------------------
\subsection{The Model}

A hidden markove model consists of $K$ latent states $\Z = \{1, 2, \ldots,
K\}$. State $k$ generates an observation based on a pre-specified distribution,
which could be multinomial, Gaussian, or any other user-defined distributions. 

\pa{1ex}{Observation.}
In our model, each state $\z_n$ generates observations along both the textual
and geographical dimensions. We use $\x_n = (\o_n, \y_n)$ to denote the observations at
timestamp $n$, where $\o_n$ is a two-dimensional vector that represents the
latitude and longitude; and $\y_n$ is a $V$-dimensional vector that represents
the keywords using the bag-of-words model ($V$ is the size of vocabulory).

\begin{itemize}
  \item
    For the textual part, we assume the words $\y_n$ are generated from a
    multinomial distribution, namely
    $$
    p(\y_n | \z_n = k) = \Prod_{i=1}^V \theta_{ki}^{\y_{ni}}.
    $$
    where $\theta_{ki}$ is the probability for state $k$ to generate word $i$.

  \item
    For the geographical part, we assume the location $\o_n$ is generated from a Gaussian mixture
    that has $M$ components, namely
    $$
    p(\o_n | \z_n = k) = \Sum_{l=1}^M c_{kl} b_{kl}(x_l),
    $$
    where $b_{kl}(x)$ is the normal distribution with parameters $\mu_{kl}$ and $\Sigma_{kl}$, \ie,
    $b_{kl}(x) = \N(\mu_{kl}, \Sigma_{kl})$; and $c_{kl}$ is the probability for choosing Guassian component $l$ in state $k$.

\end{itemize}

Also, we assume the textual observations and the geographical observations are generated
independently, hence
$$
p(\o_n, \y_n | \z_n = k)  = p(\o_n | z_n = k) \cdot p(\y_n | \z_n = k).
= \prod_{i=1}^V \theta_{kv}^{x_v}   \Sum_{l=1}^M c_{kl} b_{kl}(x_l),
$$


\pa{1ex}{Transition.}
For a length-$N$ sequence $\{(\o_1, \y_1), (\o_2, \y_2),
\ldots, (\o_N, \y_N)\}$, each observation $(\o_n, \y_n) (1 \le n \le N)$ corresponds to a
latent state $\z_n \in \Z$.  There is a prior distribution $\pi$ over the
latent state $\z_j$, namely $\pi_k = p(\z_1 = k)$. In addition, the transitions
between the latent states are characterized by a $K \times K$ matrix $\A$.
Suppose the latent sate at timestamp $n-1$ is $j$, the probability for the
latent state at timestamp $n$ to be $k$ is $\A_{jk} = p(\z_n = k | \z_{n-1} =
j)$. Note that the next state only depends on the current state  and is 
time-homogeneous (the transition matrix $\A$ does not change with time).


% Based on the above, the probability of 
% $$
% p(\x_1, \x_2, \ldots, \x_N, \z_1, \z_2, \ldots, \z_N} = p(\z_1) 
% $$



\pa{1ex}{Parameters.}
To sum up, the hidden Markov model is characterized by the parameters $\Theta = \{\pi, \A, \phi \}$.

\begin{itemize}
  \item
    $\pi$: the probability distribution that the object resides on the latent states
    at the initial step, namely $\pi_k = p(\z_1 = k)$. Clearly, $\Sum_{k=1}^K \pi_k = 1$.
  \item
    $\A$ is a $K \times K$ transition matrix, the element $\A_{jk}$ represents the probability of
    transiting from state $j$ to state $k$.
  \item
    $\phi$ is a set of parameters that govern the distribution of the observations. In our case,
    the observations are generated by a mixture of Gaussians (the geographical part) and a 
    multinomial (the textual part). Thus, $\phi$ includes the following parameters for each state $k$:
    $\theta_{ki}$ for $1 \le i \le V$; $c_{kl}$ and $\mu_{kl}$ and $\Sigma_{kl}$ for $1 \le l \le M$.
\end{itemize}




%-------------------------------------------------------------------------------
% Learning the parameters using E-M.
%-------------------------------------------------------------------------------

\subsection{Parameter Inference}
The EM algorithm can be used to infer the parameters of our model.

\subsubsection{E-Step.}

In the E-step, we need to compute the probability of latent variables, assuming
we are given a set of old parameters $\Theta^0$. We define the following
quatities.

\begin{itemize}
  \item
    The probability distribution of the latent state at position $n$:
    $$
    \gamma(\z_n) = p(\z_n | \X, \Theta^0).
    $$
    For example, $\gamma(\z_{nk})$ is 
    the probability that the latent state at position $n$ is $k$.

  \item
    The probability distribution of the Gaussian component for the latent
    variable at position $n$:
    $$
    \gamma(\c_{nk}) = p(\c_{nk} | \X, \Theta^0).
    $$
    For example, $\gamma(\c_{nkl})$ is the probability
    that the latent state at position $n$ is $k$, and the Gaussian component is $l$.

  \item
    The probability of transiting from a state at position $n-1$ to a
    state at position $n$:
    $$
    \xi(\z_{n-1}, \z_n) = p(\z_{n-1}, \z_n | \X, \Theta^0).
    $$
    For example, $\xi(\z_{n-1, j}, \z_{nk})$ is the
    probability that the latent state is $j$ at position $n-1$, and state $k$ at
    position $n$.

\end{itemize}


The efficient computation of $\gamma(\z_{nk})$ and $\gamma(\c_{nkl})$:

The computation of $\gamma(\z_{nk})$ can be achieved using the forward-backward algorithm.
$$
\gamma(\z_{n}) = \frac{\alpha(\z_n) \beta(\z_n)}{p(\X)},
$$
where 
\begin{align*}
  \alpha(\z_n)  & =  p(\x_1, \x_2, \ldots, \x_n, \z_n) \\
  \beta(\z_n)  & =  p(\x_{n+1}, \ldots, \x_N | \z_n)
\end{align*}
Both quantities can be evaluated iteratively, namely
$$
\alpha(\z_n) = p(\x_n | \z_n) \Sum_{\z_{n-1}} \alpha(\z_{n-1}) p(\z_n | \z_{n-1}),
$$
and
$$
\alpha(\z_1) = p(\x_1, \z_1) = p(\z_1) p(\x_1 | \z_1) = \Prod_{k=1}^K \{\pi_k p(\x_1 | \phi_k)\}^{\z_{1k}},
$$
namely
$$
\alpha(\z_1 = k) = \pi_k p(\x_1 | \phi_k).
$$

$$
\beta(\z_n) = \Sum_{\z_{n+1}} \beta(\z_{n+1}) p(\x_{n+1} | \z_{n+1}) p(\z_{n+1} | \z_{n}),
$$
and
$$
\beta(\z_N) = 1.
$$
for all settings of $\z_N$.


$$
\gamma(\c_{nkl}) = \gamma(\z_{nk}) \frac{c_{kl} b_{kl}(\o_n)}{\Sum_{l=1}^M c_{kl} b_{kl}(\o_n)}.
$$
% Two sets of latent variables:
% $\z$ is the latent variable that indicates which states the object is at;
% $\l$ is the latent variable that indicates which Gaussin component the location $\o_n$ is generated from.

\subsubsection{M-Step.}

In the M-step, the inferred latent variable values are used to update the parameters
by maximizing the complete data likelihood.

To begin with, since $\z_{nk}$ is a binary variable (1 indicates the latent state at $n$ is $k$, and 0 indicates
the latent sate is not $k$), we have
$$
\Sum_{\z} p(\z | \X, \Theta^0) \cdot \z_{nk} = p(\z_{nk} = 1 | \X, \Theta^0) = \gamma(\z_{nk}),
$$
and
$$
\Sum_{\z} p(\z | \X, \Theta^0) \cdot \z_{n-1, j} \cdot \z_{nk} = \xi(\z_{n-1, j}, \z_{nk}).
$$


To find the updated parameters $\Theta$, we write down the Q-function as
\begin{align*}
  L(\theta)
  = & \Sum_{\z} \Sum_{\c} p(\z, \c| \X, \Theta^0) \cdot \ln p(\X, \z, \c| \Theta) \\
  = & \Sum_{\z} p(\z | \X, \Theta^0) \ln p(\z_1 | \pi) \\
  & +  \Sum_{\z} p(\z | \X, \Theta^0) \Sum_{n=2}^N \ln p(\z_n | \z_{n-1}, \A) \\
  & +  \Sum_{\z} \Sum_{\c} p(\z, \c| \X, \Theta^0) \Sum_{n=1}^N \Sum_{k=1}^K \z_{nk} \ln p(\o_n | \phi_k)
  +  \Sum_{\z} \Sum_{\c} p(\z, \c| \X, \Theta^0) \Sum_{n=1}^N \Sum_{k=1}^K \z_{nk} \ln p(\y_n | \phi_k) \\
  = & \Sum_{\z} p(\z | \X, \Theta^0) \ln p(\z_1 | \pi) \\
  & +  \Sum_{\z} p(\z | \X, \Theta^0) \Sum_{n=2}^N \ln p(z_n | z_{n-1}, \A) \\
  & +  \Sum_{\z} \Sum_{\c} p(\z, \c| \X, \Theta^0) \Sum_{n=1}^N \Sum_{k=1}^K \z_{nk} \ln p(\o_n | \phi_k)
  +  \Sum_{\z} p(\z| \X, \Theta^0) \Sum_{n=1}^N \Sum_{k=1}^K \z_{nk} \ln p(\y_n | \phi_k) \\
\end{align*}

Note that, the first two terms are derived because the latent variable $\c$ is
marginalized. Similarly, for the fourth term, since the quantity is irrelevant to the latent 
variable $c$, marginalization of $p(\z, \c| \X, \Theta^0)$ can be done.

Since the four terms are independent, we can optimize
them separately.

% The term $\o_n$ represents the geographical observation, and the term $\y_n$ represents the textual observation.

\pa{1ex}{Optimization.}\\
Consider the first term, since the latent variable $\z_1$ can take a value from
$\{1, 2, \ldots, K\}$, we have 
$$
p(\z_1 | \pi) = \Prod_{k=1}^K \pi_k^{\z_{1k}}.
$$

Hence, the first term in the above equation becomes 
$$
f = & \Sum_{\z} p(\z | \X, \Theta^0) \Sum_{k=1}^K \z_{1k} \ln \pi_k \\
= & \Sum_{k=1}^K \gamma(\z_{1k}) \ln \pi_k.
$$

Using the Lagrange multiplier, the maximum of the above is achieved when
$$
\pi_k = \frac{\gamma(\z_{1k})} {\Sum_{j=1}^K \gamma(\z_{1j})}.
$$


For the second term, we have
\begin{align*}
  f = & \Sum_{\z} p(\z | \X, \Theta^0) \Sum_{n=2}^N \Sum_{k=1}^K \Sum_{j=1}^K \z_{n-1, j} \z_{nk} \ln A_{jk} \\
  = & \Sum_{n=2}^N \Sum_{k=1}^K \Sum_{j=1}^K \xi(\z_{n-1, j}, \z_{nk}) \ln A_{jk} \\
\end{align*}

Similarly, we can use the Lagrange multiplier to obtain the optimal parameter as
$$
A_{jk} = \frac{\Sum_{n=2}^N \xi(\z_{n-1, j}, \z_{nk} )}
{\Sum_{n=2}^N \Sum_{i=1}^K \xi(\z_{n-1, j}, \z_{ni} )}
$$



Now we consider the fourth term:
$$
f = \Sum_{n=1}^N \Sum_{k=1}^K \gamma(\z_{nk}) \ln p(\y_n | \phi_k)
$$
% $$
% f = \Sum_{n=1}^N \Sum_{k=1}^K \gamma(\z_{nk}) \ln p(\o_n, \y_n | \phi_k)
% $$
where $\y_n$ represents the textual
observation. Recall that, we model the generation of the words
as the observations from multinomial distribution. Let the vacabulory size be $V$, the for state 
$k$, we have
$$
p(\y_n | \phi_k) = \Prod_{i=1}^V \theta_{ki}^{\y_{ni}}
$$
then the objective function becomes
$$
f = \Sum_{n=1}^N \Sum_{k=1}^K \Sum_{i=1}^V \gamma(\z_{nk}) \y_{ni} \ln \theta_{ki}
$$

Again, using the Lagrange multiplier, we obtain the parameter estimation as
$$
\theta_{ki} = \frac{\Sum_{n=1}^N \y_{ni} \gamma(\z_{nk})}
{\Sum_{n=1}^N \gamma(\z_{nk})}.
$$

We proceed to consider the geographical part. Recall that we assume the geographical location $\o_n$ is
generated from a Gaussian mixture $\phi_k$ that have $M$ components, namely
$$
p(\o_n | \phi_k) = \Sum_{l=1}^M c_{kl} b_{kl}(\o_n)
$$
the objective function is now
$$
f = \Sum_{n=1}^N \Sum_{k=1}^K \Sum_{l=1}^M \gamma(\c_{nkl}) \ln(c_{kl} b_{kl}(\o_n))
$$

The optimal parameters are computed as
\begin{align*}
  c_{kl} = & \frac{\Sum_{n=1}^N \gamma(\c_{nkl})}{\Sum_{n=1}^N \gamma(\z_{nk})} \\
  \mu_{kl} = & \frac{\Sum_{n=1}^N \gamma(\c_{nkl}) \o_n}{\Sum_{n=1}^N \gamma(\c_{nkl})} \\
  \Sigma_{kl} = & \frac{\Sum_{n=1}^N \gamma(\c_{nkl}) (\o_n - \mu_{kl}) (\o_n - \mu_{kl})^T }{\Sum_{n=1}^N \gamma(\c_{nkl})} \\
\end{align*}

%-------------------------------------------------------------------------------
% Extending the previous model into a mixture of two models.
%-------------------------------------------------------------------------------
\subsection{Mixture Model.}


Now we consider to extend the HMM model into a mixture model, where we have a HMM model 
parameterized by $\Theta$ and a background model parameterized by $\Phi$. The prior probability
of choosing the HMM model is $\lambda ~(0 < \lambda < 1)$. Given a length-2
sequence $\X$, we associate a binary latent variable $\h$ with $\x$, $\h=1$ indicates $\x$ is 
generated from HMM, and $\h=0$ means $\x$ is generated from the background model. Under the extended
mixture model, we can again use the EM algorithm to infer all the parameters.

Note that $
 p(\X, \z, \c| \h=0, \Phi) = 
 p(\X| \h=0, \Phi)$ because for the background model, there is no latente variables.

The Q-function is 
\begin{align*}
  L(\theta)
  = & \Sum_{\h} \Sum_{\z} \Sum_{\c} p(\h, \z, \c| \X, \Theta^0) \cdot \ln p(\X, \h, \z, \c| \Theta) \\
  = & \Sum_{\h} \Sum_{\z} \Sum_{\c} p(\h, \z, \c| \X, \Theta^0) \cdot
  \{h \ln [\lambda p(\X, \z, \c| \Theta)] + (1 - h) \ln [ (1 - \lambda) p(\X, \z, \c| \Phi)] \}\\
  = &  \Sum_{\z} \Sum_{\c} p(\h = 1, \z, \c| \X, \Theta^0) 
  \{ \ln \lambda  + \ln p(\X, \z, \c| \h=1, \Theta) \}\\
  &  + \Sum_{\z} \Sum_{\c} p(\h = 0, \z, \c| \X, \Theta^0) 
  \{ \ln (1 - \lambda)  + \ln p(\X, \z, \c| \h=0, \Phi) \} \\
  = &  \Sum_{\z} \Sum_{\c} p(\h = 1, \z, \c| \X, \Theta^0) \ln \lambda
     + \Sum_{\z} \Sum_{\c} p(\h = 1, \z, \c| \X, \Theta^0) \ln p(\X, \z, \c| \h=1, \Theta) \\
    & + \Sum_{\z} \Sum_{\c} p(\h = 0, \z, \c| \X, \Theta^0) \ln (1 - \lambda)
    + \Sum_{\z} \Sum_{\c} p(\h = 0, \z, \c| \X, \Theta^0) \ln p(\X| \h=0, \Phi) \\
  = &  p(\h = 1| \X, \Theta^0) \ln \lambda + p(\h = 0| \X, \Theta^0) \ln (1 - \lambda) \\
   &  + \Sum_{\z} \Sum_{\c} p(\h = 1, \z, \c| \X, \Theta^0) \ln p(\X, \z, \c| \h=1, \Theta) \\
    & +  p(\h = 0| \X, \Theta^0) \ln p(\X| \h=0, \Phi)
  % = & \Sum_{\z} p(\z | \X, \Theta^0) \ln p(\z_1 | \pi) \\
  % & +  \Sum_{\z} p(\z | \X, \Theta^0) \Sum_{n=2}^N \ln p(\z_n | \z_{n-1}, \A) \\
  % & +  \Sum_{\z} \Sum_{\c} p(\z, \c| \X, \Theta^0) \Sum_{n=1}^N \Sum_{k=1}^K \z_{nk} \ln p(\o_n | \phi_k)
  % +  \Sum_{\z} \Sum_{\c} p(\z, \c| \X, \Theta^0) \Sum_{n=1}^N \Sum_{k=1}^K \z_{nk} \ln p(\y_n | \phi_k) \\
  % = & \Sum_{\z} p(\z | \X, \Theta^0) \ln p(\z_1 | \pi) \\
  % & +  \Sum_{\z} p(\z | \X, \Theta^0) \Sum_{n=2}^N \ln p(z_n | z_{n-1}, \A) \\
  % & +  \Sum_{\z} \Sum_{\c} p(\z, \c| \X, \Theta^0) \Sum_{n=1}^N \Sum_{k=1}^K \z_{nk} \ln p(\o_n | \phi_k)
  % +  \Sum_{\z} p(\z| \X, \Theta^0) \Sum_{n=1}^N \Sum_{k=1}^K \z_{nk} \ln p(\y_n | \phi_k) \\
\end{align*}


\subsubsection{E-Step.}
In the E-step, we need to compute the latent variables.
$$
p(\h=1 | \X, \Theta^0) = \frac{\lambda p(\X | \Theta_{HMM}}
{\lambda p(\X | \Theta_{HMM} +  (1- \lambda) p(\X | \Phi)}
$$

The latent variables $\z$ and $\c$ are exactly the same using the formulas
defined in the previous section.
$$
\gamma(\z_n) = p(\z_n | \X, \h=1, \Theta^0)
$$
and similarly $\gamma(\c_{nk})$ and $\xi(\z_{n-1}, \z_n)$.


\subsubsection{M-Step.}
In the Q-function, the first line can be used to obtain the optimal $\lambda$ as 
$$
\lambda = p(\h=1 | \X, \Theta^0).
$$

The last term is a constant, we can ignore it during optimization. Now consider the second
line in the Q-function, we have
$$
L(\Theta) = p(\h = 1| \X, \Theta^0)
\Sum_{\z} \Sum_{\c} p(\z, \c| \X, \h=1, \Theta^0) \ln p(\X, \z, \c| \h=1, \Theta),
$$
when there is only one sequence $\X$, the optimal parameters for HMM are exactly the same
as the original ones.



%-------------------------------------------------------------------------------
% Extension to multiple sequences.
%-------------------------------------------------------------------------------
\subsection{Extension to Multiple Sequences.}

Now we consider that we have multiple sequences $\X^1, \X^2, \ldots, \X^R$. The Q-function now becomes

\begin{align*}
  L(\Theta)
  = & \Sum_{r=1}^R \Sum_{\h^r} \Sum_{\z^r} \Sum_{\c^r} p(\h^r, \z^r, \c^r| \X^r, \Theta^0)
  \ln p(\X^r, \h^r, \z^r, \c^r| \Theta) \\
  = & \Sum_{r=1}^R \Sum_{\h^r} \Sum_{\z^r} \Sum_{\c^r} p(\h^r, \z^r, \c^r| \X^r, \Theta^0)
  \{\h^r \ln [\lambda p(\X^r, \z^r, \c^r| \Theta)] + (1 - \h^r) \ln [ (1 - \lambda) p(\X^r, \z^r, \c^r| \Phi)] \}\\
  = &  \Sum_{r=1}^R \Sum_{\z^r} \Sum_{\c^r} p(\h^r = 1, \z^r, \c^r| \X^r, \Theta^0) 
  \{ \ln \lambda  + \ln p(\X^r, \z^r, \c^r| \h^r=1, \Theta) \}\\
  &  + \Sum_{r=1}^R \Sum_{\z^r} \Sum_{\c^r} p(\h^r = 0, \z^r, \c^r| \X^r, \Theta^0) 
  \{ \ln (1 - \lambda)  + \ln p(\X^r, \z^r, \c^r| \h^r=0, \Phi) \} \\
  = &  \Sum_{r=1}^R \Sum_{\z^r} \Sum_{\c^r} p(\h^r = 1, \z^r, \c^r| \X^r, \Theta^0) \ln \lambda
     + \Sum_{r=1}^R \Sum_{\z^r} \Sum_{\c^r} p(\h^r = 1, \z^r, \c^r| \X^r, \Theta^0) \ln p(\X^r, \z^r, \c^r| \h^r=1, \Theta) \\
    & + \Sum_{r=1}^R \Sum_{\z^r} \Sum_{\c^r} p(\h^r = 0, \z^r, \c^r| \X^r, \Theta^0) \ln (1 - \lambda)
    + \Sum_{r=1}^R \Sum_{\z^r} \Sum_{\c^r} p(\h^r = 0, \z^r, \c^r| \X^r, \Theta^0) \ln p(\X^r| \h^r=0, \Phi) \\
  = & \Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \ln \lambda + \Sum_{r=1}^R p(\h^r = 0| \X^r, \Theta^0) \ln (1 - \lambda) \\
   &  + \Sum_{r=1}^R \Sum_{\z^r} \Sum_{\c^r} p(\h^r = 1, \z^r, \c^r| \X^r, \Theta^0) \ln p(\X^r, \z^r, \c^r| \h^r=1, \Theta) \\
    & + \Sum_{r=1}^R p(\h^r = 0| \X^r, \Theta^0) \ln p(\X^r| \h^r=0, \Phi)
\end{align*}


In the E-step, the latent variables $\h^r, \z^r, \c^r$ are computed similarly.

In the M-step, the estimations of the parameters are as follows:
$$
\lambda = \frac{\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) }{R}.
$$

Consider the second
line in the Q-function, we have
\begin{align*}
L(\Theta) = &
\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{\z^r} \Sum_{\c^r} p( \z^r, \c^r| \X^r, \h^r = 1,\Theta^0) \ln p(\X^r, \z^r, \c^r| \h^r=1, \Theta) \\
= & \Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{\z^r} p(\z^r | \X^r, \Theta^0) \ln p(\z^r_1 | \pi) \\
& + \Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{\z^r} p(\z^r | \X^r, \Theta^0) \Sum_{n=2}^N \ln p(\z^r_n | \z^r_{n-1}, \A) \\
& + \Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{\z^r} \Sum_{\c^r} p(\z^r, \c^r| \X^r, \Theta^0) \Sum_{n=1}^N \Sum_{k=1}^K \z^r_{nk} \ln p(\o_n | \phi_k) \\
& +\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0)  \Sum_{\z^r} \Sum_{\c^r} p(\z^r, \c^r| \X^r, \Theta^0) \Sum_{n=1}^N \Sum_{k=1}^K \z^r_{nk} \ln p(\y_n | \phi_k) \\
% = &\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{\z^r} p(\z^r | \X^r, \Theta^0) \ln p(\z^r_1 | \pi) \\
% & +\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0)  \Sum_{\z^r} p(\z^r | \X^r, \Theta^0) \Sum_{n=2}^N \ln p(z_n | z_{n-1}, \A) \\
% & +\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0)  \Sum_{\z^r} \Sum_{\c^r} p(\z^r, \c^r| \X^r, \Theta^0) \Sum_{n=1}^N \Sum_{k=1}^K \z^r_{nk} \ln p(\o_n | \phi_k) \\
% & +\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0)  \Sum_{\z^r} p(\z^r| \X^r, \Theta^0) \Sum_{n=1}^N \Sum_{k=1}^K \z^r_{nk} \ln p(\y_n | \phi_k)
\end{align*}

Similarly, the parameters can be estimated as follows:
$$
\pi_k = \frac{\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \gamma(\z_{1k}^r)} 
{\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{j=1}^K  \gamma(\z_{1j}^r)} 
$$

$$
A_{jk} = \frac{ \Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{n=2}^N \xi(\z_{n-1, j}^r, \z_{nk}^r )}
{ \Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{n=2}^N \Sum_{i=1}^K \xi(\z_{n-1, j}^r, \z_{ni}^r )}
$$

$$
\theta_{ki} = \frac{\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0)  \Sum_{n=1}^N \y_{ni}^r \gamma(\z_{nk}^r)}
{\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{n=1}^N \gamma(\z_{nk}^r)}.
$$


$$
  c_{kl} =  \frac{\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{n=1}^N \gamma(\c_{nkl}^r)}
  {\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{n=1}^N \gamma(\z_{nk}^r)}
$$

$$
  \mu_{kl} = \frac{\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{n=1}^N \gamma(\c_{nkl}^r) \o_n^r}
  {\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{n=1}^N \gamma(\c_{nkl}^r)}
$$

$$
  \Sigma_{kl} =
  \frac{\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{n=1}^N \gamma(\c_{nkl}^r) (\o_n^r - \mu_{kl}) (\o_n^r - \mu_{kl})^T }
  {\Sum_{r=1}^R p(\h^r = 1| \X^r, \Theta^0) \Sum_{n=1}^N \gamma(\c_{nkl}^r)}
$$



%-------------------------------------------------------------------------------
% The background model.
%-------------------------------------------------------------------------------

\subsection{The Background Model.}

The background model is an extension of the Gaussian mixture model such that each
component generates a Gaussian as well as a number of keywords from the vacabulory.
Note that, with a large enough number of components, this background model can well
capture the cases where the geographical topic distribution is non-Gaussian.

We consider the input $\X = \{\x_1, \x_2, \ldots, \x_N\}$, where are $N$ places such
that each place $\x_n$ is described by a location $\o_n$ and a word vector $\y_n$.
We use EM to obtain the parameters for this background model.

Let
$$
\gamma(\z_{nk}) = p(\z_{nk} = 1 | \X, \Theta^0).
$$

Note that
$$
\ln p(\x_n, \z_n) = \Sum_{k=1}^K \z_{nk} [\ln \pi_k + \ln p(\x_n | \phi_k) ]
$$

The Q-function is
\begin{align*}
Q = & \Sum_{n=1}^N \Sum_{\z_n} \ln p(\x_n, \z_n | \Theta) p(\z_n | \x_n, \Theta^0) \\
 = & \Sum_{n=1}^N \Sum_{k=1}^K \gamma(\z_{nk}) [\ln \pi_k + \ln p(\x_n | \phi_k) ]
\end{align*}

The parameters are estimated as

$$
\pi_k = \frac{1}{N} \Sum_{n=1}^N \gamma(\z_{nk})
$$


For the text parameters:
$$
\theta_{kv} = \frac{\Sum_{n=1}^N \gamma(\z_{nk}) \y_{nv}}
{\Sum_{n=1}^N \Sum_{v=1}^V \gamma(\z_{nk}) \y_{nv}}
$$

For the geographical parameters:
$$
\mu_{k} = \frac{\Sum_{n=1}^N \gamma(\z_{nk}) \o_n}
{\Sum_{n=1}^N \gamma(\z_{nk})}
$$

$$
\Sigma_{k} = \frac{\Sum_{n=1}^N \gamma(\z_{nk}) (\o_n - \mu_k) (\o_n - \mu_k)^T}
{\Sum_{n=1}^N \gamma(\z_{nk})}
$$

