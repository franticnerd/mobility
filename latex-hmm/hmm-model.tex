%! TEX root = main-hmm.tex

\section{Hidden Markove Model}

%-------------------------------------------------------------------------------
% Defining the hidden Markov model.
%-------------------------------------------------------------------------------

Consider a sequence $\{(\x_1, \y_1), (\x_2, \y_2), \ldots, (\x_N, \y_N)\}$. We
use $\o_n = (\x_n, \y_n)$ to denote the observations at timestamp $n$, where
$\x_n$ is a two-dimensional vector that represents the latitude and longitude;
and $\y_n$ is a $V$-dimensional vector that represents the keywords using the
bag-of-words model ($V$ is the size of vocabulory).  Each observation $(\x_n,
\y_n) (1 \le n \le N)$ corresponds to a latent state $\z_n \in \Z = \{1, 2,
\ldots, K\}$.  Each state $\z_n$ generates observations along both the textual
and geographical dimensions. 

\pa{1ex}{Transition.} There is a prior distribution $\pi$ for the initial
latent state $\z_1$, \ie, $\pi_k = p(\z_1 = k)$. In addition, the transitions
between the latent states are characterized by a $K \times K$ matrix $\A$.
Suppose the latent sate at timestamp $n-1$ is $j$, the probability for the
latent state at timestamp $n$ to be $k$ is $\A_{jk} = p(\z_n = k | \z_{n-1} =
j)$. Note that the next state only depends on the current state  and is
time-homogeneous (the transition matrix $\A$ does not change with time).

\pa{1ex}{Observation.}
\begin{itemize}
  \item
    For the textual part, we assume the words $\y_n$ are generated from a
    multinomial distribution, namely
    $$
    p(\y_n | \z_n = k) = \Prod_{v=1}^V \theta_{kv}^{y_{nv}}.
    $$
    where $\theta_{kv}$ is the probability for state $k$ to generate word $v$.

  \item
    For the geographical part, we assume the location $\x_n$ is generated from a Gaussian mixture
    that has $M$ components, namely
    $$
    p(\x_n | \z_n = k) = \Sum_{m=1}^M c_{km} b_{km}(\x_n),
    $$
    where $b_{km}(\x)$ is the normal distribution with parameters $\mu_{km}$ and $\Sigma_{km}$, \ie,
    $b_{km}(x) = \N(\mu_{km}, \Sigma_{km})$; and $c_{km}$ is the probability for choosing Guassian 
    component $m$ in state $k$.

\end{itemize}

Also, we assume the textual observations and the geographical observations are generated
independently, hence
$$
p(\x_n, \y_n | \z_n = k)  = p(\x_n | \z_n = k) \cdot p(\y_n | \z_n = k).
$$


% \pa{1ex}{Parameters.}
% \begin{itemize}
%   \item
%     $\pi$: the probability distribution that the object resides on the latent states
%     at the initial step, namely $\pi_k = p(\z_1 = k)$. Clearly, $\Sum_{k=1}^K \pi_k = 1$.
%   \item
%     $\A$ is a $K \times K$ transition matrix, the element $\A_{jk}$ represents the probability of
%     transiting from state $j$ to state $k$.
%   \item
%     $\phi$ is a set of parameters that govern the distribution of the observations. In our case,
%     the observations are generated by a mixture of Gaussians (the geographical part) and a 
%     multinomial (the textual part). Thus, $\phi$ includes the following parameters for each state $k$:
%     $\theta_{ki}$ for $1 \le i \le V$; $c_{km}$ and $\mu_{km}$ and $\Sigma_{km}$ for $1 \le l \le M$.
% \end{itemize}
